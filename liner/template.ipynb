{"cells":[{"cell_type":"markdown","metadata":{"id":"5N5RuFIhveel"},"source":["# Homework assignment 1: Markov Chain"]},{"cell_type":"markdown","metadata":{"id":"H1UaCc1Qveen"},"source":["## Problem 1\n","Read the article in https://www.geeksforgeeks.org/markov-chains-in-nlp/, and answer the following questions."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CBof0iUZveeo"},"source":["* (a) What are N-grams of an input sequence?\n","    * N-grams of an input sequence are contiguous sequences of N words or characters from that input. For example, in the sentence \"I love cats\", the 2-grams (bigrams) would be \"I love\" and \"love cats\", while the 3-gram (trigram) would be \"I love cats\"\n","\n","* (b) How to determine the probability of each element in the transition matrix?\n","    * The probability of each element in the transition matrix is determined by counting the frequency of specific state transitions. Specifically, for a transition from state i to state j, we calculate the number of times the sequence moves from state i to state j, divided by the total number of transitions from state i to any state.\n","\n","* (c) If you want to increase the variety of the sequence generation (every time the outputs are different), what kinds of properties the training texts should be?\n","    * Large sample size、Diverse vocabulary、Various contexts、Moderate sentence lengths"]},{"cell_type":"markdown","metadata":{"id":"IW3fVQRdveeo"},"source":["## Problem 2\n","Try the following codes, and answer questions."]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[],"source":["# Install the required packages\n","# !pip install nltk\n","# !pip install numpy"]},{"cell_type":"code","execution_count":173,"metadata":{"executionInfo":{"elapsed":2579,"status":"ok","timestamp":1725465452265,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"3rvAjDpaveep"},"outputs":[],"source":["import re\n","from nltk import ngrams\n","import numpy as np\n","from typing import List, Tuple, Union, Dict\n","from itertools import product"]},{"cell_type":"code","execution_count":174,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1725465464133,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"fuUPhTVWveeq"},"outputs":[],"source":["def words_to_index(words: List[str], base: int, unique_words: Dict[str, int]) -> int:\n","    \"\"\"\n","        Convert a list of words to an index representation.\n","\n","        Args:\n","            words (List[str]): The list of words to be converted.\n","            base (int): The base value used for conversion.\n","            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","\n","        Returns:\n","            - int: The index representation of the given list of words.\n","        \n","        Example:\n","            >>> words_to_index([\"the\", \"quick\", \"brown\", \"fox\"], 10, {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3})\n","            123\n","    \"\"\"\n","    length = len(words)\n","    numbers = [unique_words[word] for word in words]\n","    return sum([num * (base ** (length - 1 - idx)) for idx, num in enumerate(numbers)])\n","\n","def index_to_words(index: int, base: int, length: int, unique_words: Dict[str, int]):\n","    \"\"\"\n","        Converts an index to a list of words based on a given base and unique words.\n","\n","        Args:\n","            index (int): The index to convert.\n","            base (int): The base used for conversion.\n","            length (int): The length of the resulting list.\n","            unique_words (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","\n","        Returns:\n","            List[str]: A list of words corresponding to the given index.\n","\n","        Examples:\n","            >>> unique_words = {'apple': 0, 'banana': 1, 'cherry': 2}\n","            >>> index_to_words(5, 3, 2, unique_words)\n","            ['banana', 'cherry'] (Because 5 = 1 * 3^1 + 2 * 2^0)\n","\n","            >>> unique_words = {'red': 0, 'green': 1, 'blue': 2}\n","            >>> index_to_words(2, 3, 1, unique_words)\n","            ['blue'] (Because 2 = 2 * 3^0)\n","    \"\"\"\n","    numbers = []\n","    \n","    unique_words_list = list(unique_words.keys())\n","    for pow in range(length - 1, -1, -1):\n","        numbers.append(index // (base ** pow))\n","        index -= numbers[-1] * (base ** pow)\n","    \n","    return [unique_words_list[num] for num in numbers]"]},{"cell_type":"markdown","metadata":{"id":"3q89RN21veer"},"source":["### Step 1: Remove some unnecessary characters"]},{"cell_type":"code","execution_count":175,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1725465482890,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"-bLwYWvDveer"},"outputs":[],"source":["def remove_unnecessary_characters(text: str):\n","    \"\"\"\n","        Removes unnecessary characters from the given text and converts it to lowercase.\n","\n","        Args:\n","            text (str): The input text to be processed.\n","        Returns:\n","            str: The processed text with unnecessary characters removed and converted to lowercase.\n","    \"\"\"\n","    # Remove unnecessary characters\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"dDPRDP_Dveer"},"source":["### Step 2: Generate N-grams"]},{"cell_type":"code","execution_count":176,"metadata":{"executionInfo":{"elapsed":570,"status":"ok","timestamp":1725465486350,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"_XqvOwjOvees"},"outputs":[],"source":["def generate_n_grams(text: str, n: int):\n","    \"\"\"\n","        Generate n-grams from the given text.\n","\n","        Args:\n","            text (str): The input text from which n-grams will be generated.\n","            n (int): The number of consecutive words in each n-gram.\n","        Returns:\n","            List[Tuple[str]]: A list of tuples representing the generated n-grams.\n","    \"\"\"\n","    # Generate n-grams, using the ngrams function from the nltk library\n","    n_grams = ngrams(text.split(), n)\n","\n","    # Convert to list of tuples\n","    n_grams = list(n_grams)\n","\n","    return n_grams"]},{"cell_type":"markdown","metadata":{"id":"RLrCGQuyvees"},"source":["### Step 3: Compute Transition Matrix"]},{"cell_type":"code","execution_count":177,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1725465500278,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"ij_Pxsn6vees"},"outputs":[],"source":["def compute_transition_matrix(n_grams: List[Tuple[str]]):\n","    \"\"\"\n","        Computes the transition matrix and unique states for a given list of n-grams.\n","            \n","        Args:\n","            n_grams (List[Tuple[str]]): A list of n-grams, where each n-gram is a tuple of strings.\n","        Returns:\n","            np.ndarray: transition matrix.\n","            Dict[str, int]: A dictionary mapping unique words to their corresponding indices.\n","    \"\"\"\n","    \n","    # Get the value of n\n","    n = len(n_grams[0])\n","    \n","    ## Step 3.1: Collect all possible words (label words with indices)\n","    \n","    unique_words = {}\n","    unique_words_count = 0\n","    \n","    # A helper function to add a word to the unique_words dictionary, if it is not already present\n","    def _add_to_unique_word(word: str):\n","        # We need to increase the counter outside this function\n","        nonlocal unique_words_count\n","\n","        # Check if the state is already in the unique_states dictionary\n","        if unique_words.get(word, None) is None:\n","            unique_words[word] = unique_words_count\n","            unique_words_count += 1\n","    \n","    # Iterate over all n-grams to collect all possible words\n","    for n_gram in n_grams:\n","        for word in n_gram:\n","            # [TODO] Add the word to the unique_words dictionary\n","            # hint: use the _add_to_unique_word function\n","            _add_to_unique_word(word)\n","            continue\n","    \n","    \n","    ## Step 3.2: Compute the transition matrix\n","    \n","    # In this part, we use `words_to_index` and `index_to_words` functions to convert between state name (words) and indices\n","    \n","    # Example:\n","    # If we have 3 unique words: 0 for \"apple\", 1 for \"banana\", 2 for \"cherry\", and n = 3,\n","    # then the state names are \"apple, apple\", \"apple, banana\", \"apple, cherry\", \"banana, apple\", \"banana, banana\", and so on.\n","    # In this case, the mapping from states to indices would be:\n","    # \"(apple, apple)\" <-> 0, \"(apple, banana)\" <-> 1, \"(apple, cherry)\" <-> 2, \"(banana, apple)\" <-> 3, and so on.\n","    \n","    # [TODO] Compute the number of states in Markov chain. Number of states is the number of unique words to the power of n-1\n","    unique_states_count = unique_words_count ** (n-1)\n","    \n","    # [TODO] Create transition matrix, filled with zeros (the size of the transition matrix is # states x # states)\n","    transition_matrix = np.zeros((unique_states_count, unique_states_count))\n","\n","\n","    # Count the number of transitions from each state to another state\n","    for n_gram in n_grams:\n","        # [TODO] Find the indices of the states (from and to) (i.e. the first n-1 words and the last n-1 words)\n","        # hint: use words_to_index function\n","        state_from_index = words_to_index(n_gram[:-1], unique_words_count, unique_words)\n","        state_to_index = words_to_index(n_gram[1:], unique_words_count, unique_words)\n","        # Increment the count of the transition from state_from to state_to\n","        transition_matrix[state_from_index][state_to_index] += 1\n","        # print(f\"From: {n_gram[:-1]} (index: {state_from_index}), To: {n_gram[1:]} (index: {state_to_index})\")\n","\n","\n","    # Special case:\n","    # if the sum of number of transition of a state is zero, which means it has no information about the next state,\n","    # we set the probabilities of transitioning to all possible next states to 1 / # words\n","    \n","    # Check if any row sums to zero\n","    row_sums_is_zero = transition_matrix.sum(axis=1) == 0\n","    \n","    # Iterate over rows to find zero-sum rows\n","    first_word = list(unique_words.keys())[0]\n","    for idx, is_zero in enumerate(row_sums_is_zero):\n","        # If the row sum is zero\n","        if is_zero:\n","            # Convert the index to state representation\n","            state_name = index_to_words(idx, unique_words_count, n, unique_words)\n","            \n","            # Calculate the range of indices for the next posiible states\n","            possible_states_start_index = words_to_index(state_name[1:] + [first_word, ], unique_words_count, unique_words)\n","            possible_states_end_index = possible_states_start_index + unique_words_count\n","            \n","            # Set the number of transitions to 1 for the range\n","            transition_matrix[idx, possible_states_start_index : possible_states_end_index] = 1\n","\n","    ## Step 3.3: Normalize the transition matrix\n","    \n","    # The following steps is to normalize transition matrix so that the sum of each row is 1\n","    # (The reason is: By the definition of a Markov chain, the sum of each row should be 1)\n","    \n","    # [TODO] Compute the sum of each row\n","    # hint: use np.sum function and check if the axis parameter is set correctly\n","    row_sums = row_sums = np.sum(transition_matrix, axis=1)\n","\n","\n","    # [TODO] Divide the transition matrix by the sum of each row to make the sum of each row equals to 1\n","    n = transition_matrix.shape[1]  \n","    transition_matrix = np.divide(transition_matrix, row_sums[:, np.newaxis], where=row_sums[:, np.newaxis] != 0)\n","    # 對於行和為0的行，設置所有元素為1/n\n","    transition_matrix[row_sums == 0] = 1 / n\n","\n","    return transition_matrix, unique_words"]},{"cell_type":"markdown","metadata":{"id":"rLiaaGcpveet"},"source":["### Step 4: Generate text"]},{"cell_type":"code","execution_count":178,"metadata":{"executionInfo":{"elapsed":641,"status":"ok","timestamp":1725465504726,"user":{"displayName":"黃莉婷","userId":"05775179293410531427"},"user_tz":-480},"id":"Bh7NXbk4veet"},"outputs":[],"source":["def generate(unique_words: Dict[str, int], transition_matrix: np.ndarray, start_from: Union[str, List[str]], n: int, length: int=10):\n","    \"\"\"\n","        Generate text using a Markov chain model.\n","\n","        Args:\n","            unique_states (Dict[str, int]): A dictionary mapping unique words to their corresponding indices.\n","            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n","            start_from (Union[str, List[str]]): The starting state(s) for text generation.\n","            n (int): The size of the grams.\n","            length (int, optional): The length of the generated text. Defaults to 10.\n","\n","        Returns:\n","            The generated text.\n","    \"\"\"\n","    # Generate text\n","    generated_words = start_from.copy() if type(start_from) is list else [start_from]\n","    \n","    # Assert if the number of start words does not equal to n-1\n","    assert len(generated_words) >= n-1, \"The number of start words should be greater than or equals to n-1 ({})\".format(n-1)\n","\n","    # Get the number of unique words    \n","    unique_words_count = len(unique_words)\n","    \n","    # [TODO] Get the number of unique states\n","    # hint: check step 3.2 in the compute_transition_matrix function\n","    unique_states_count = len(unique_words) ** (n-1)\n","\n","    # Generate the next words\n","    for _ in range(length):\n","        # [TODO] Get index of current states\n","        # hint: The current states (current words) is the last n-1 words in the generated text\n","        # hint: use words_to_index function\n","        current_words_index = words_to_index(generated_words[-(n-1):], len(unique_words), unique_words)\n","\n","        # [TODO] Get probability distribution for next state, using the information in the transition matrix\n","        probabilities = transition_matrix[current_words_index]\n","        #print(f\"Probabilities: {probabilities}\")\n","\n","\n","        # Select next word based on probabilities, using np.random.choice function\n","        next_words_index = np.random.choice(unique_states_count, p=probabilities)\n","\n","        # [TODO] Decode the index and get the last word\n","        # hint: use index_to_words function\n","        next_word = index_to_words(next_words_index, unique_words_count, n, unique_words)[-1]\n","       # print(next_word)\n","\n","        # Add next word to generated text\n","        generated_words.append(next_word)\n","\n","    # return generated string\n","    return ' '.join(generated_words)"]},{"cell_type":"code","execution_count":179,"metadata":{"id":"KkOugSjwveeu"},"outputs":[],"source":["text = \"\"\"The cat sat on the mat. The dog barked at the cat. The cat ran away from the dog. The dog chased the cat. The cat jumped over the mat. The mat lay still as the cat jumped. The dog sat on the mat after the cat left. The cat looked back at the dog on the mat. The dog barked at the jumping cat. The mat was there for the cat to sit. The cat sat on the same mat. The dog barked at the same cat.\"\"\"\n","\n","n = 3\n","# Process the text and generate the transition matrix\n","text = remove_unnecessary_characters(text)\n","n_grams = generate_n_grams(text, n)\n","transition_matrix, unique_words = compute_transition_matrix(n_grams)"]},{"cell_type":"code","execution_count":180,"metadata":{"id":"4MAL08Egveeu","outputId":"4b6e5c82-80d2-46f9-cde7-76f3047dae53"},"outputs":[{"name":"stdout","output_type":"stream","text":["===== The indices for unique states are: =====\n","the,the   : 0\n","the,cat   : 1\n","the,sat   : 2\n","the,on    : 3\n","the,mat   : 4\n","the,dog   : 5\n","the,barked: 6\n","the,at    : 7\n","the,ran   : 8\n","the,away  : 9\n","the,from  : 10\n","the,chased: 11\n","the,jumped: 12\n","the,over  : 13\n","the,lay   : 14\n","the,still : 15\n","the,as    : 16\n","the,after : 17\n","the,left  : 18\n","the,looked: 19\n","...\n","\n","===== The transition matrix is (Shape of trasition matrix: (784, 784)): =====\n","[[0.03571429 0.03571429 0.03571429 ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n"," [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n"," [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]]\n","\n"]}],"source":["# Print the transition matrix and unique states for obsevation\n","print(\"===== The indices for unique states are: =====\")\n","unique_words_count = len(unique_words)\n","for word_name in list(product(*[unique_words for _ in range(n-1)]))[:20]:\n","    print(f\"{','.join(word_name):10s}: {words_to_index(word_name, unique_words_count, unique_words)}\")\n","print(\"...\", end=\"\\n\\n\")\n","\n","print(\"===== The transition matrix is (Shape of trasition matrix: {}): =====\".format(transition_matrix.shape))\n","print(transition_matrix)\n","print()"]},{"cell_type":"code","execution_count":201,"metadata":{"id":"rnbUL1itveev","outputId":"ea20fe90-5d66-4c76-ae61-32287f57848c"},"outputs":[{"name":"stdout","output_type":"stream","text":["#1 (length=25): The cat sat on the mat the mat lay still as the cat the mat the dog chased the cat the cat the cat jumped the dog barked at\n","#2 (length=30): The mat lay still as the cat the cat sat on the mat the dog chased the cat to sit the cat the mat after the cat sat on the mat after the\n","#3 (length=35): The dog barked at the same mat the dog the dog sat on the mat lay still as the cat jumped over the mat the dog on the same mat the dog barked at the dog chased the\n"]}],"source":["# [TODO] Write down 3 or more initial words and length of generated text to start the text generation\n","\n","experiments = [\n","    \n","    ('The cat sat on', 25), \n","    ('The mat lay', 30), \n","    ('The dog barked', 35),  \n","]\n","\n","for idx, (start_from, length) in enumerate(experiments, 1):\n","    start_from = start_from.split(\" \")\n","\n","    # Generate text using the transition matrix\n","    generated_text = generate(unique_words, transition_matrix, start_from, n, length=length)\n","    \n","    # Print out the generated text\n","    print(\"#{} (length={}): {}\".format(idx, length, generated_text))"]},{"cell_type":"markdown","metadata":{"id":"YrK8ourT6aE2"},"source":["### Answer the following questions"]},{"cell_type":"markdown","metadata":{"id":"nrUZ91rpvcPt"},"source":["* (a) Write a new text of at least 15 words as the input.\n","    -  The cat sat on the mat. The dog barked at the cat. The cat ran away from the dog. The dog chased the cat. The cat jumped over the mat. The mat lay still as the cat jumped. The dog sat on the mat after the cat left. The cat looked back at the dog on the mat. The dog barked at the jumping cat. The mat was there for the cat to sit. The cat sat on the same mat. The dog barked at the same cat.\n","\n","* (b) Run the program 3 times with different output length and different initial words. Show the outputs.\n","    - #1 (length=25): The cat sat on the mat was there for the cat jumped the dog on the mat lay still as the cat to sit the cat sat on the\n","    -  #2 (length=30): The mat lay still as the cat jumped over the mat the mat the mat lay still as the cat sat on the mat lay still as the cat sat on the same\n","    -  #3 (length=35): The dog barked at the jumping cat the cat left the cat to sit the cat sat on the mat lay still as the cat jumped over the mat after the cat the cat jumped over the mat\n","     \n","* (c) Try different N of N-grams. How the N influences the output sequence?\n","    * [TODO]"]},{"cell_type":"markdown","metadata":{"id":"0Ej2DuDTveev"},"source":["## Problem 3\n","The Stationary Distribution of a Markov chain is a distribution of probabilities that remains unchanged after a transition from one state to another."]},{"cell_type":"markdown","metadata":{"id":"8EGILVlKvcPu"},"source":["* (a) Ask an LLM (Large Language Model), such as ChatGPT, what are the applications of stationary distribution of a Markov chain. You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs).\n","    * [TODO]\n","* (b) Ask an LLM, such as ChatGPT, what numerical method is the most efficient approach to compute the stationary distribution? You need to show which prompts are used, and state how you verify the correctness of the results (output by LLMs)\n","    * [TODO]\n","* (c) Implement the method suggested by the LLM. Use the transition matrix generated in question 2.a as an input to compute its stationary distribution.\n","    * Please implememt the method `compute_stationary_distribution` below\n","* (d) The theory of probability matrix is given in the textbook 6.8, Eigenvalues/eigenvector of nonegtive matrices. Read textbook 6.8 and ask two questions that you are curious most about it\n","    * [TODO]"]},{"cell_type":"code","execution_count":182,"metadata":{"id":"-VwWF0P1veev"},"outputs":[],"source":["def compute_stationary_distribution(transition_matrix: np.ndarray):\n","    \"\"\"\n","        Compute the stationary distribution of a Markov chain.\n","\n","        Parameters:\n","            transition_matrix (np.ndarray): A numpy array representing the transition probabilities between states.\n","        Returns:\n","            The stationary distribution of the Markov chain.\n","    \"\"\"\n","   \n","    n = transition_matrix.shape[0]\n","    initial_distribution = np.ones(n) / n  \n","    epsilon = 1e-8  \n","    max_iterations = 1000  \n","    \n","    for _ in range(max_iterations):\n","        new_distribution = initial_distribution @ transition_matrix\n","        if np.allclose(new_distribution, initial_distribution, atol=epsilon):\n","            return new_distribution\n","        initial_distribution = new_distribution\n","    \n","    \n","    return initial_distribution\n","    stationary_distribution = None\n","    return stationary_distribution"]},{"cell_type":"code","execution_count":183,"metadata":{"id":"-owTykzFveev","outputId":"9ade2e5c-14a9-4043-c38d-85e3b20bd7a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.0002184 0.08954   0.0002184 0.0002184 0.05798   0.0688    0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.01015   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0198\n"," 0.02847   0.0002106 0.01811   0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.00916   0.0002106 0.0002106 0.0002106 0.01811   0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.00916   0.00916   0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.00916   0.0002106 0.0002106\n"," 0.0002184 0.0002184 0.0002184 0.02837   0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0386    0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0393    0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.00988   0.0002106 0.0002106 0.00988   0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.00988   0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.01003   0.0002106 0.01003   0.01003   0.0002106 0.0002106 0.02968\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.01003   0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.02989   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0397    0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.009384  0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0096    0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.00982   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.010254  0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.00928   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.00928\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.00949   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.01009   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.010315  0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.01053   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.01009   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.009384  0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.009384\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0096    0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002106 0.01036   0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.01009   0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.010315  0.0002184 0.0002184 0.0002184\n"," 0.01053   0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.009384  0.0002184\n"," 0.0096    0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184 0.0002184\n"," 0.0002106 0.01011   0.0002106 0.0002106 0.01011   0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106\n"," 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106 0.0002106]\n"]}],"source":["# Compute the stationary distribution for the transition matrix obtained from the previous problem\n","stationary_distribution = compute_stationary_distribution(transition_matrix)\n","\n","# Convert the stationary distribution to float16 to prevent something like -1.2345678e-16 but it is actually 0\n","print(stationary_distribution.astype(np.float16))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
